{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f15f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FoMAC_Autoformer.ipynb\n",
    "from MarkovAutoformer import MarkovAutoformer, MarkovAutoformerConfig\n",
    "from markov_autoformer_pipeline import prepare_data, MarkovAutoformerTrainer, calculate_metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import load, dump\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd1632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTIONS\n",
    "class TrendLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.8):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.L1Loss()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pred, true):\n",
    "        loss_val = self.l1(pred, true)\n",
    "        trend_penalty = torch.mean(\n",
    "            (torch.sign(pred[:, 1:, :] - pred[:, :-1, :]) != \n",
    "             torch.sign(true[:, 1:, :] - true[:, :-1, :])).float()\n",
    "        )\n",
    "        return loss_val + self.alpha * trend_penalty\n",
    "\n",
    "\n",
    "class HybridHuberTrendLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.8, beta=0.5, delta=1.0):\n",
    "        super().__init__()\n",
    "        self.huber = nn.HuberLoss(delta=delta)\n",
    "        self.trend = TrendLoss(alpha)\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, pred, true):\n",
    "        huber_loss = self.huber(pred, true)\n",
    "        trend_loss = self.trend(pred, true)\n",
    "        return (1 - self.beta) * huber_loss + self.beta * trend_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc0ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedMarkovAutoformerTrainer(MarkovAutoformerTrainer):\n",
    "    \n",
    "    def train_epoch(self, train_loader, optimizer, criterion):\n",
    "        self.model.train()\n",
    "        total_main_loss = 0.0\n",
    "        total_markov_loss = 0.0\n",
    "        total_combined_loss = 0.0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            seq_x, seq_x_mark, seq_y, seq_y_mark, seq_y_target, seq_state_x, seq_state_y = batch\n",
    "            \n",
    "            seq_x = seq_x.to(self.device)\n",
    "            seq_x_mark = seq_x_mark.to(self.device)\n",
    "            seq_y = seq_y.to(self.device)\n",
    "            seq_y_mark = seq_y_mark.to(self.device)\n",
    "            seq_y_target = seq_y_target.to(self.device)\n",
    "            seq_state_x = seq_state_x.to(self.device)\n",
    "            seq_state_y = seq_state_y.to(self.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = self.model(seq_x, seq_x_mark, seq_y, seq_y_mark,\n",
    "                               seq_state_x=seq_state_x, seq_state_y=seq_state_y)\n",
    "            \n",
    "            if outputs.shape[-1] > 1:\n",
    "                outputs = outputs[..., 0:1]\n",
    "            \n",
    "            pred = outputs if not isinstance(outputs, (tuple, list)) else outputs[0]\n",
    "            \n",
    "            pred_len = min(pred.shape[1], seq_y_target.shape[1])\n",
    "            pred = pred[:, -pred_len:, :]\n",
    "            target = seq_y_target[:, -pred_len:, :]\n",
    "            main_loss = criterion(pred, target)\n",
    "            \n",
    "            markov_loss = self._compute_state_supervision_loss(seq_state_x)\n",
    "            \n",
    "            weight = getattr(self.model, 'markov_supervised_weight', 0.3)\n",
    "            loss = main_loss + weight * markov_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_main_loss += main_loss.item()\n",
    "            total_markov_loss += markov_loss.item()\n",
    "            total_combined_loss += loss.item()\n",
    "        \n",
    "        return {\n",
    "            'combined': total_combined_loss / max(1, len(train_loader)),\n",
    "            'main': total_main_loss / max(1, len(train_loader)),\n",
    "            'markov': total_markov_loss / max(1, len(train_loader))\n",
    "        }\n",
    "    \n",
    "    def validate(self, val_loader, criterion):\n",
    "        self.model.eval()\n",
    "        total_main_loss = 0.0\n",
    "        total_markov_loss = 0.0\n",
    "        total_combined_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                seq_x, seq_x_mark, seq_y, seq_y_mark, seq_y_target, seq_state_x, seq_state_y = batch\n",
    "                \n",
    "                seq_x = seq_x.to(self.device)\n",
    "                seq_x_mark = seq_x_mark.to(self.device)\n",
    "                seq_y = seq_y.to(self.device)\n",
    "                seq_y_mark = seq_y_mark.to(self.device)\n",
    "                seq_y_target = seq_y_target.to(self.device)\n",
    "                seq_state_x = seq_state_x.to(self.device)\n",
    "                seq_state_y = seq_state_y.to(self.device)\n",
    "                \n",
    "                outputs = self.model(seq_x, seq_x_mark, seq_y, seq_y_mark,\n",
    "                                   seq_state_x=seq_state_x, seq_state_y=seq_state_y)\n",
    "                \n",
    "                if outputs.shape[-1] > 1:\n",
    "                    outputs = outputs[..., 0:1]\n",
    "                \n",
    "                pred = outputs if not isinstance(outputs, (tuple, list)) else outputs[0]\n",
    "                pred_len = min(pred.shape[1], seq_y_target.shape[1])\n",
    "                pred = pred[:, -pred_len:, :]\n",
    "                target = seq_y_target[:, -pred_len:, :]\n",
    "                main_loss = criterion(pred, target)\n",
    "                \n",
    "                markov_loss = self._compute_state_supervision_loss(seq_state_x)\n",
    "                weight = getattr(self.model, 'markov_supervised_weight', 0.3)\n",
    "                loss = main_loss + weight * markov_loss\n",
    "                \n",
    "                total_main_loss += main_loss.item()\n",
    "                total_markov_loss += markov_loss.item()\n",
    "                total_combined_loss += loss.item()\n",
    "        \n",
    "        return {\n",
    "            'combined': total_combined_loss / max(1, len(val_loader)),\n",
    "            'main': total_main_loss / max(1, len(val_loader)),\n",
    "            'markov': total_markov_loss / max(1, len(val_loader))\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPARATION\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPARING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "train_loader, val_loader, test_loader, scalers_dict, feature_cols = prepare_data(\n",
    "    features_path='feature_engineered_data.csv',\n",
    "    seq_len=120,\n",
    "    label_len=14,\n",
    "    pred_len=14,\n",
    "    batch_size=16,\n",
    "    stride=1,\n",
    "    train_ratio=0.8,\n",
    "    val_ratio=0.1,\n",
    "    location='MANILA CITY',\n",
    ")\n",
    "\n",
    "# Filter to most relevant features\n",
    "selected_features = [\n",
    "    # Epidemic\n",
    "    'cases_minmax', 'cases_minmax_lag_1w', 'cases_minmax_lag_2w',\n",
    "    'cases_minmax_lag_3w', 'cases_minmax_lag_4w', 'cases_minmax_roll_mean_4w',\n",
    "    \n",
    "    # Rainfall\n",
    "    'RAINFALL_minmax', 'RAINFALL_minmax_lag_1w', 'RAINFALL_minmax_lag_2w',\n",
    "    'RAINFALL_minmax_lag_3w', 'RAINFALL_minmax_roll_mean_4w',\n",
    "    'RAIN_L6', 'RAIN_L8', 'RAIN_L12',\n",
    "    \n",
    "    # Temperature\n",
    "    'TMAX_minmax', 'TMIN_minmax', 'TMAX_minmax_lag_1w', 'TMIN_minmax_lag_1w',\n",
    "    'TMAX_minmax_roll_mean_4w', 'TMIN_minmax_roll_mean_4w', 'TMAX_x_RH',\n",
    "    'TMAX_L6', 'TMAX_L8', 'TMAX_L12',\n",
    "    \n",
    "    # Wind\n",
    "    'WIND_SPEED_minmax', 'WIND_DIR_X', 'WIND_DIR_Y',\n",
    "    \n",
    "    # Humidity\n",
    "    'RH_minmax', 'RH_minmax_lag_1w', 'RH_minmax_roll_mean_4w',\n",
    "    'RH_L6', 'RH_L8', 'RH_L12',\n",
    "    \n",
    "    # Time encodings\n",
    "    'month_sin', 'month_cos', 'week_of_year_sin', 'week_of_year_cos'\n",
    "]\n",
    "\n",
    "feature_cols = [f for f in feature_cols if f in selected_features]\n",
    "\n",
    "print(f\"\\n✓ Using {len(feature_cols)} selected features\")\n",
    "print(f\"✓ Train batches: {len(train_loader)}\")\n",
    "print(f\"✓ Val batches: {len(val_loader)}\")\n",
    "print(f\"✓ Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CONFIGURATION\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFIGURING MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "config = MarkovAutoformerConfig()\n",
    "\n",
    "# Sequence parameters\n",
    "config.seq_len = 120\n",
    "config.label_len = 14\n",
    "config.pred_len = 14\n",
    "\n",
    "# Input/output dimensions\n",
    "config.enc_in = len(feature_cols)\n",
    "config.dec_in = len(feature_cols)\n",
    "config.c_out = 1\n",
    "\n",
    "# === MODEL SIZE ===\n",
    "config.d_model = 256\n",
    "config.n_heads = 8\n",
    "config.e_layers = 3\n",
    "config.d_layers = 2\n",
    "config.d_ff = 1024\n",
    "\n",
    "# Regularization\n",
    "config.dropout = 0.5\n",
    "config.moving_avg = 13\n",
    "\n",
    "# === MARKOV PARAMETERS ===\n",
    "config.n_states = 16\n",
    "config.markov_weight = 0.4\n",
    "config.markov_tau = 0.8\n",
    "\n",
    "# === STATE SUPERVISION ===\n",
    "config.markov_supervised_weight = 0.4\n",
    "\n",
    "config.embed = 'timeF'\n",
    "config.freq = 'w'\n",
    "config.activation = 'gelu'\n",
    "config.output_attention = False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = MarkovAutoformer(config)\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "print(f\"\\n✓ Model initialized on {device}\")\n",
    "print(f\"✓ Total trainable parameters: {total_params:,}\")\n",
    "print(f\"✓ Expected memory: ~{total_params * 4 / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f8d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING SETUP\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING SETUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "trainer = EnhancedMarkovAutoformerTrainer(model, device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    return min(1.0, (epoch + 1) / 20)\n",
    "warmup_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "main_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=30, T_mult=2\n",
    ")\n",
    "\n",
    "criterion = HybridHuberTrendLoss(alpha=0.8, beta=0.6, delta=1.0)\n",
    "\n",
    "epochs = 200\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = {'combined': [], 'main': [], 'markov': []}\n",
    "val_losses = {'combined': [], 'main': [], 'markov': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9032104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING LOOP\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STARTING TRAINING (WITH STATE SUPERVISION)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "patience = 15       \n",
    "counter = 0         \n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train\n",
    "    train_loss_dict = trainer.train_epoch(train_loader, optimizer, criterion)\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_dict = trainer.validate(val_loader, criterion)\n",
    "    model.train()\n",
    "    \n",
    "    # Scheduler\n",
    "    if epoch < 10:\n",
    "        warmup_scheduler.step()\n",
    "    else:\n",
    "        main_scheduler.step(epoch)\n",
    "    \n",
    "    # Track losses\n",
    "    for key in train_losses.keys():\n",
    "        train_losses[key].append(train_loss_dict[key])\n",
    "        val_losses[key].append(val_loss_dict[key])\n",
    "    \n",
    "    # Compute 3-epoch moving average for validation\n",
    "    if len(val_losses['combined']) >= 3:\n",
    "        recent_avg = sum(val_losses['combined'][-3:]) / 3\n",
    "    else:\n",
    "        recent_avg = val_loss_dict['combined']\n",
    "    \n",
    "    # --- CHECK FOR IMPROVEMENT ---\n",
    "    if recent_avg < best_val_loss - 1e-5:\n",
    "        best_val_loss = recent_avg\n",
    "        torch.save(model.state_dict(), 'best_markov_autoformer.pth')\n",
    "        improvement_flag = \"✓ IMPROVED\"\n",
    "        counter = 0  # Reset counter on success\n",
    "    else:\n",
    "        counter += 1 # Increment counter on failure\n",
    "        improvement_flag = f\"  (Wait {counter}/{patience})\"\n",
    "    \n",
    "    # Logging\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0 or counter >= patience:\n",
    "        print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n",
    "              f\"Train: {train_loss_dict['combined']:.5f} \"\n",
    "              f\"(Main: {train_loss_dict['main']:.5f}, State: {train_loss_dict['markov']:.5f}) | \"\n",
    "              f\"Val: {val_loss_dict['combined']:.5f} \"\n",
    "              f\"(Main: {val_loss_dict['main']:.5f}, State: {val_loss_dict['markov']:.5f}) \"\n",
    "              f\"{improvement_flag}\")\n",
    "\n",
    "    # --- BREAK IF PATIENCE EXCEEDED ---\n",
    "    if counter >= patience:\n",
    "        print(\"\\n\" + \"!\" * 40)\n",
    "        print(f\"EARLY STOPPING TRIGGERED AT EPOCH {epoch+1}\")\n",
    "        print(\"!\" * 40)\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"✓ Training complete! Best val loss: {best_val_loss:.6f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c64398",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "# Plot training & validation loss\n",
    "ax.plot(train_losses['combined'], label='Training Loss', linewidth=2)\n",
    "ax.plot(val_losses['combined'], label='Validation Loss', linewidth=2)\n",
    "\n",
    "# Unified title\n",
    "ax.set_title('Training and Validation Loss over Epochs')\n",
    "\n",
    "# Unified axis labels\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "\n",
    "ax.ticklabel_format(style='sci', axis='y', scilimits=(0, 0))\n",
    "\n",
    "# Legend\n",
    "ax.legend()\n",
    "\n",
    "# Visible grid\n",
    "ax.grid(True, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2259b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model.load_state_dict(torch.load('best_markov_autoformer.pth'))\n",
    "predictions, targets = trainer.predict(test_loader)\n",
    "\n",
    "print(f\"✓ Predictions shape: {predictions.shape}\")\n",
    "print(f\"✓ Targets shape: {targets.shape}\")\n",
    "\n",
    "# Inverse transform to original scale\n",
    "predictions = np.array(predictions)\n",
    "targets = np.array(targets)\n",
    "\n",
    "df = pd.read_csv('feature_engineered_data.csv')\n",
    "cases_raw = df['cases'].values.reshape(-1, 1)\n",
    "\n",
    "scaler_cases = MinMaxScaler()\n",
    "scaler_cases.fit(cases_raw)\n",
    "\n",
    "pred_target = predictions[..., 0:1]\n",
    "true_target = targets[..., 0:1]\n",
    "\n",
    "pred_target = np.clip(pred_target, 0.0, 1.0)\n",
    "\n",
    "pred_real = scaler_cases.inverse_transform(pred_target.reshape(-1, 1)).reshape(pred_target.shape)\n",
    "true_real = scaler_cases.inverse_transform(true_target.reshape(-1, 1)).reshape(true_target.shape)\n",
    "\n",
    "\n",
    "# Compute metrics\n",
    "def compute_metrics(true_vals, pred_vals):\n",
    "    true_vals = true_vals.reshape(-1)\n",
    "    pred_vals = pred_vals.reshape(-1)\n",
    "    \n",
    "    mse = mean_squared_error(true_vals, pred_vals)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(true_vals, pred_vals)\n",
    "    r2 = r2_score(true_vals, pred_vals)\n",
    "    smape = 100 * np.mean(2 * np.abs(pred_vals - true_vals) / \n",
    "                          (np.abs(true_vals) + np.abs(pred_vals) + 1e-8))\n",
    "    corr = np.corrcoef(true_vals, pred_vals)[0, 1]\n",
    "    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2, 'SMAPE': smape, 'Correlation': corr}\n",
    "\n",
    "metrics_real = compute_metrics(true_real, pred_real)\n",
    "\n",
    "print(\"\\nMETRICS ON ORIGINAL DENGUE CASE COUNTS:\")\n",
    "for k, v in metrics_real.items():\n",
    "    print(f\"  {k:12s}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nMETRICS ON SCALED (0–1) CASE VALUES:\")\n",
    "\n",
    "pred_scaled = pred_target.reshape(-1)\n",
    "true_scaled = true_target.reshape(-1)\n",
    "\n",
    "scaled_mse = mean_squared_error(true_scaled, pred_scaled)\n",
    "scaled_rmse = np.sqrt(scaled_mse)\n",
    "scaled_mae = mean_absolute_error(true_scaled, pred_scaled)\n",
    "\n",
    "print(f\"  MSE (scaled):   {scaled_mse:.4f}\")\n",
    "print(f\"  RMSE (scaled):  {scaled_rmse:.4f}\")\n",
    "print(f\"  MAE (scaled):   {scaled_mae:.4f}\")\n",
    "print(f\"  Range of pred_scaled: {pred_scaled.min():.4f} to {pred_scaled.max():.4f}\")\n",
    "print(f\"  Range of true_scaled: {true_scaled.min():.4f} to {true_scaled.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c82ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION\n",
    "\n",
    "true_real = np.array(true_real).flatten()\n",
    "pred_real = np.array(pred_real).flatten()\n",
    "\n",
    "# Actual vs Predicted\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(true_real[:200], label='Actual', linewidth=2.5, color='black', alpha=0.7)\n",
    "plt.plot(pred_real[:200], label='Predicted', linewidth=2, linestyle='--', color='dodgerblue')\n",
    "plt.title('Dengue Cases Prediction (With State Supervision)', fontsize=14)\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Dengue Cases')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Training and evaluation complete!\")\n",
    "print(\"✓ States were used for supervision throughout training\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f932142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "seq_len = 120\n",
    "pred_len = 52\n",
    "stride = 1\n",
    "\n",
    "# Fake timeline (just integers for visualization)\n",
    "timeline = np.arange(1, 200)\n",
    "\n",
    "# Windows for visualization (first 3 windows)\n",
    "win1 = timeline[0:seq_len]\n",
    "win2 = timeline[stride:stride+seq_len]\n",
    "win3 = timeline[2*stride:2*stride+seq_len]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "\n",
    "# Plot horizontal bars to represent windows\n",
    "ax.plot(win1, np.ones_like(win1)*3, label=\"Window 1: Weeks 1–120\", linewidth=8)\n",
    "ax.plot(win2, np.ones_like(win2)*2, label=\"Window 2: Weeks 2–121\", linewidth=8)\n",
    "ax.plot(win3, np.ones_like(win3)*1, label=\"Window 3: Weeks 3–122\", linewidth=8)\n",
    "\n",
    "# Formatting\n",
    "ax.set_title(\"Sliding Window Visualization (Stride = 1)\", fontsize=14)\n",
    "ax.set_xlabel(\"Week Index\")\n",
    "ax.set_yticks([1,2,3])\n",
    "ax.set_yticklabels([\"Window 3\",\"Window 2\",\"Window 1\"])\n",
    "ax.grid(True, linestyle='--', alpha=0.4)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
